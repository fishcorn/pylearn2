hyper_parameters.train_dataset = "!obj:pylearn2.datasets.libsvm.LibSVMDataset { dataset_name: 'cod', which_set: 'train', rng_seed: %(rng_seed)d }"
hyper_parameters.y_lr_scale = 1.0
hyper_parameters.momentum_final = 0.99
jobman.start_time = 1453664386.80703
jobman.end_time = 1453664574.252508
hyper_parameters.momentum_saturate = 100
hyper_parameters.momentum_start = 1
hyper_parameters.h0_lr_scale = 1.0
hyper_parameters.h0_sd = 0.03125
yaml_template = '!obj:pylearn2.train.Train {\n  dataset: &train %(train_dataset)s,\n\n  model: !obj:pylearn2.models.mlp.MLP {\n    nvis: %(n_vis)d,\n    layers: [\n      !obj:pylearn2.models.mlp.Cos {\n        layer_name: \'h0\',\n        dim: %(h0_width)d,\n        istdev: %(h0_sd)f,\n        W_lr_scale: %(h0_lr_scale)f,\n        b_lr_scale: %(h0_lr_scale)f,\n      },\n      !obj:pylearn2.models.mlp.Softmax {\n        layer_name: \'y\',\n        n_classes: %(n_classes)d,\n        irange: 0.01,\n        W_lr_scale: %(y_lr_scale)f,\n        b_lr_scale: %(y_lr_scale)f,\n      }\n    ],\n    monitor_targets: 1,\n    batch_size: &batch_size %(batch_size)d,\n  },\n\n  algorithm: !obj:pylearn2.training_algorithms.sgd.SGD {\n    batch_size: *batch_size,\n    learning_rate: %(learning_rate)f,\n    learning_rule: !obj:pylearn2.training_algorithms.learning_rule.Momentum {\n      init_momentum: %(momentum_initial)f,\n    },\n    train_iteration_mode: \'even_sequential\',\n    monitor_iteration_mode: \'even_sequential\',\n    monitoring_batches: 5,\n    monitoring_dataset: {\n      train: *train,\n      valid : &valid %(valid_dataset)s,\n    },\n\n    termination_criterion: !obj:pylearn2.termination_criteria.And\n    { criteria: [\n\n        !obj:pylearn2.termination_criteria.MonitorBased { \n          channel_name: \'valid_objective\',\n          prop_decrease: 0.,\n          N: %(early_stop_iters)d,\n        },\n\n        !obj:pylearn2.termination_criteria.EpochCounter {\n          max_epochs: %(max_epochs)d,\n        },\n\n    ]},\n\n  },\n\n  extensions: [\n\n    !obj:pylearn2.train_extensions.best_params.MonitorBasedSaveBest {\n      channel_name: \'valid_objective\',\n      save_path: "lsh8_ckl_best.pkl",\n    },\n    !obj:pylearn2.training_algorithms.learning_rule.MomentumAdjustor {\n      start: %(momentum_start)d,\n      saturate: %(momentum_saturate)d,\n      final_momentum: %(momentum_final)f\n    },\n  ], \n\n  save_freq: 10,\n  save_path: "lsh8_ckl_model.pkl"\n}\n'
hyper_parameters.learning_rate = 8.058634622656107e-05
hyper_parameters.max_epochs = 1000
hyper_parameters.batch_size = 64
jobman.time = 'Sun Jan 24 12:39:45 2016'
jobman.experiment = 'pylearn2_experiments.lsh.ckl_trainer.train_experiment'
hyper_parameters.early_stop_iters = 100
hyper_parameters.n_classes = 2
jobman.run_time = 187.4454779624939
hyper_parameters.n_vis = 8
extract_results = 'pylearn2_experiments.utils.results_extractor'
hyper_parameters.valid_dataset = "!obj:pylearn2.datasets.libsvm.LibSVMDataset { dataset_name: 'cod', which_set: 'valid', rng_seed: %(rng_seed)d }"
hyper_parameters.momentum_initial = 0.5
hyper_parameters.h0_width = 32.0
hyper_parameters.rng_seed:=@numpy.random.randint(0, 1000000000)
